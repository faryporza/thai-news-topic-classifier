#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thai News Topic Classifier - Backend API (ONNX Edition)
=======================================================
Flask API à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸³à¹à¸™à¸à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸”à¹‰à¸§à¸¢ ONNX Runtime

Endpoints:
- GET /health - à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸° API
- GET /model/info - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸¡à¹€à¸”à¸¥
- POST /predict - à¸—à¸³à¸™à¸²à¸¢à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹ˆà¸²à¸§

Model: farypor/my-thai-news-classifier-onnx (ONNX on Hugging Face Hub)
- à¸„à¸§à¸²à¸¡à¹à¸¡à¹ˆà¸™à¸¢à¸³: ~95-100%
- Inference à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² PyTorch 2-3x
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ GPU
"""

import os
import re
import time
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS
from datetime import datetime
from huggingface_hub import snapshot_download
import onnxruntime as ort
from transformers import AutoTokenizer

# ============================================================================
# Flask App Configuration
# ============================================================================
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# ============================================================================
# Load Model from Hugging Face Hub (ONNX)
# ============================================================================
REPO_ID = "farypor/my-thai-news-classifier-onnx"
LOCAL_DIR = "onnx_model"
MAX_LENGTH = 256

# Global model and tokenizer
tokenizer = None
session = None

# Label mapping
ID2LABEL = {0: "Business", 1: "SciTech", 2: "World"}
LABEL2ID = {"Business": 0, "SciTech": 1, "World": 2}

model_info = {
    "name": "Thai News Topic Classifier",
    "version": "2.1.0",
    "algorithm": "WangchanBERTa + ONNX",
    "model_type": "ONNX Runtime",
    "model_id": REPO_ID,
    "base_model": "airesearch/wangchanberta-base-att-spm-uncased",
    "classes": ["Business", "SciTech", "World"],
    "created_at": "2026-01-31",
    "description": "à¹‚à¸¡à¹€à¸”à¸¥à¸ˆà¸³à¹à¸™à¸à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ 3 à¸«à¸¡à¸§à¸” à¸”à¹‰à¸§à¸¢ ONNX Runtime",
    "advantages": [
        "Inference à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² PyTorch 2-3x",
        "à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ GPU",
        "à¹„à¸Ÿà¸¥à¹Œà¹€à¸¥à¹‡à¸à¸¥à¸‡",
        "Deploy à¸‡à¹ˆà¸²à¸¢"
    ],
    "accuracy": "100% (à¸šà¸™ test set)",
    "max_length": MAX_LENGTH
}


def load_models():
    """à¹‚à¸«à¸¥à¸” ONNX Model à¹à¸¥à¸° Tokenizer à¸ˆà¸²à¸ Hugging Face Hub"""
    global tokenizer, session
    
    try:
        # Download model from Hugging Face Hub
        print(f"ðŸ“¥ Downloading model from {REPO_ID}...")
        snapshot_download(
            repo_id=REPO_ID,
            repo_type="model",
            local_dir=LOCAL_DIR,
            local_dir_use_symlinks=False
        )
        print(f"   âœ… Downloaded to {LOCAL_DIR}")
        
        # Load tokenizer
        tokenizer_path = os.path.join(LOCAL_DIR, "tokenizer")
        print(f"ðŸ“¥ Loading tokenizer from {tokenizer_path}...")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)
        
        # Load ONNX model
        onnx_path = os.path.join(LOCAL_DIR, "model.onnx")
        print(f"ðŸ“¥ Loading ONNX model from {onnx_path}...")
        session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
        
        print(f"âœ… ONNX model loaded successfully!")
        print(f"   - Model ID: {REPO_ID}")
        print(f"   - Provider: CPUExecutionProvider")
        print(f"   - Labels: {ID2LABEL}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return False


# Load models at module level (for gunicorn)
try:
    load_models()
except Exception as e:
    print(f"âŒ Error loading models during startup: {e}")


# ============================================================================
# Text Preprocessing
# ============================================================================
def preprocess_text(text: str) -> str:
    """
    Preprocess à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ (à¸—à¸³à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸šà¸•à¸­à¸™ training)
    """
    if text is None:
        return ""
    
    text = str(text)
    
    # 1. Whitespace Normalization
    text = re.sub(r'\s+', ' ', text)
    
    # 2. Strip
    text = text.strip()
    
    # 3. Thai Digits Normalization
    thai_digits = 'à¹à¹‘à¹’à¹“à¹”à¹•à¹–à¹—à¹˜à¹™'
    arabic_digits = '0123456789'
    for thai, arabic in zip(thai_digits, arabic_digits):
        text = text.replace(thai, arabic)
    
    return text


def softmax(x):
    """Compute softmax values"""
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)


# ============================================================================
# API Endpoints
# ============================================================================
@app.route('/')
def home():
    """Home endpoint"""
    return jsonify({
        "message": "Thai News Topic Classifier API",
        "version": "2.1.0",
        "model": "ONNX Runtime",
        "endpoints": [
            "GET /health",
            "GET /model/info",
            "POST /predict"
        ]
    })


@app.route('/health', methods=['GET'])
def health():
    """
    Health Check Endpoint
    à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸° API à¹à¸¥à¸°à¹‚à¸¡à¹€à¸”à¸¥
    """
    model_loaded = tokenizer is not None and session is not None
    
    return jsonify({
        "status": "healthy" if model_loaded else "unhealthy",
        "timestamp": datetime.now().isoformat(),
        "model_loaded": model_loaded,
        "model_type": "ONNX Runtime" if model_loaded else None,
        "provider": "CPUExecutionProvider" if model_loaded else None
    }), 200 if model_loaded else 503


@app.route('/model/info', methods=['GET'])
def get_model_info():
    """
    Model Info Endpoint
    à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸¡à¹€à¸”à¸¥
    """
    if tokenizer is None or session is None:
        return jsonify({
            "error": "Model not loaded"
        }), 503
    
    # Build info response
    info = model_info.copy()
    info["vocabulary_size"] = len(tokenizer)
    info["model_loaded"] = True
    info["id2label"] = ID2LABEL
    
    return jsonify(info)


@app.route('/predict', methods=['POST'])
def predict():
    """
    Predict Endpoint
    à¸—à¸³à¸™à¸²à¸¢à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹ˆà¸²à¸§à¸”à¹‰à¸§à¸¢ ONNX Runtime
    
    Request Body:
    {
        "headline": "à¸žà¸²à¸”à¸«à¸±à¸§à¸‚à¹ˆà¸²à¸§",
        "body": "à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¹ˆà¸²à¸§"
    }
    
    Response:
    {
        "label": "Business",
        "confidence": 0.95,
        "probabilities": {...},
        "latency_ms": 15,
        "model_version": "2.1.0",
        "model_type": "ONNX Runtime"
    }
    """
    start_time = time.time()
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¹‰à¸§à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡
    if tokenizer is None or session is None:
        return jsonify({
            "error": "Model not loaded",
            "message": "à¸à¸£à¸¸à¸“à¸²à¸£à¸­à¹‚à¸¡à¹€à¸”à¸¥à¹‚à¸«à¸¥à¸”à¹€à¸ªà¸£à¹‡à¸ˆ"
        }), 503
    
    # à¸£à¸±à¸š request data
    data = request.get_json()
    
    if not data:
        return jsonify({
            "error": "No data provided",
            "message": "à¸à¸£à¸¸à¸“à¸²à¸ªà¹ˆà¸‡ JSON body"
        }), 400
    
    headline = data.get('headline', '')
    body = data.get('body', '')
    
    if not headline and not body:
        return jsonify({
            "error": "Missing required fields",
            "message": "à¸à¸£à¸¸à¸“à¸²à¸à¸£à¸­à¸ headline à¸«à¸£à¸·à¸­ body à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 1 à¸­à¸¢à¹ˆà¸²à¸‡"
        }), 400
    
    # à¸£à¸§à¸¡ headline à¹à¸¥à¸° body
    text = headline + ' ' + body
    
    # Preprocess
    text = preprocess_text(text)
    
    # Tokenize
    inputs = tokenizer(
        text,
        return_tensors="np",
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length"
    )
    
    # Run ONNX inference
    logits = session.run(
        None,
        {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"]
        }
    )[0]
    
    # Calculate probabilities
    probs = softmax(logits)[0]
    predicted_id = int(np.argmax(probs))
    
    # Calculate latency
    end_time = time.time()
    latency_ms = round((end_time - start_time) * 1000, 2)
    
    # Build response
    predicted_label = ID2LABEL[predicted_id]
    confidence = float(probs[predicted_id])
    
    prob_dict = {
        ID2LABEL[i]: float(probs[i])
        for i in range(len(probs))
    }
    
    return jsonify({
        "label": predicted_label,
        "confidence": confidence,
        "probabilities": prob_dict,
        "latency_ms": latency_ms,
        "model_version": model_info["version"],
        "model_type": "ONNX Runtime",
        "input": {
            "headline": headline[:100] + "..." if len(headline) > 100 else headline,
            "body": body[:200] + "..." if len(body) > 200 else body
        }
    })


# ============================================================================
# Main
# ============================================================================
if __name__ == '__main__':
    print("=" * 60)
    print("ðŸ‡¹ðŸ‡­ Thai News Topic Classifier API")
    print("   Model: ONNX Runtime (v2.1.0)")
    print("=" * 60)
    
    # Load models
    if load_models():
        print(f"   Classes: {list(ID2LABEL.values())}")
        print(f"   Vocabulary size: {len(tokenizer)}")
    
    print("\nðŸ“¡ Starting server...")
    print("   URL: http://localhost:5001")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=5001, debug=True)
