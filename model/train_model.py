#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thai News Topic Classifier
===========================
à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸³à¹à¸™à¸à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆà¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (Thai News Topic Classification)

à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™:
1. Dataset Understanding - à¸—à¸³à¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
2. Preprocessing - à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
3. Baseline Model Training - à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥ TF-IDF + Logistic Regression
4. Evaluation - à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥
5. Error Analysis - à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”

Author: Thai News Classifier Team
Date: 2026-01-27
"""

import os
import re
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    confusion_matrix,
    classification_report
)
import joblib
import matplotlib.pyplot as plt
import seaborn as sns


# ============================================================================
# à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 1: Dataset Understanding (à¸à¸²à¸£à¸—à¸³à¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ Dataset)
# ============================================================================
def load_and_understand_dataset(csv_path: str) -> pd.DataFrame:
    """
    à¹‚à¸«à¸¥à¸”à¹à¸¥à¸°à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸šà¸·à¹‰à¸­à¸‡à¸•à¹‰à¸™
    
    Dataset Description:
    - à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: à¸‚à¹ˆà¸²à¸§à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ (Thai News Topic Dataset)
    - Input Features: headline (à¸à¸²à¸”à¸«à¸±à¸§) + body (à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¹ˆà¸²à¸§)
    - Target Label: topic (à¹€à¸Šà¹ˆà¸™ SciTech, World, Business)
    - à¸¥à¸±à¸à¸©à¸“à¸°à¸‚à¹‰à¸­à¸¡à¸¹à¸¥: train_easy version clean - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡à¸ªà¸°à¸­à¸²à¸” à¸„à¸§à¸²à¸¡à¸¢à¸²à¸à¸£à¸°à¸”à¸±à¸šà¸‡à¹ˆà¸²à¸¢
    
    Args:
        csv_path: à¸à¸²à¸˜à¹„à¸›à¸¢à¸±à¸‡à¹„à¸Ÿà¸¥à¹Œ CSV
        
    Returns:
        DataFrame à¸—à¸µà¹ˆà¹‚à¸«à¸¥à¸”à¸¡à¸²
    """
    print("=" * 70)
    print("à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 1: Dataset Understanding (à¸à¸²à¸£à¸—à¸³à¸„à¸§à¸²à¸¡à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ Dataset)")
    print("=" * 70)
    
    # à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    df = pd.read_csv(csv_path)
    
    print(f"\nğŸ“Š à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹ˆà¸§à¹„à¸›:")
    print(f"   - à¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§ (samples): {len(df):,}")
    print(f"   - à¸ˆà¸³à¸™à¸§à¸™à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ: {len(df.columns)}")
    print(f"   - à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ: {list(df.columns)}")
    
    print(f"\nğŸ·ï¸ à¸à¸²à¸£à¸à¸£à¸°à¸ˆà¸²à¸¢à¸‚à¸­à¸‡ Target Label (topic):")
    topic_counts = df['topic'].value_counts()
    for topic, count in topic_counts.items():
        percentage = count / len(df) * 100
        print(f"   - {topic}: {count:,} ({percentage:.1f}%)")
    
    print(f"\nğŸ“ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (3 à¹à¸–à¸§à¹à¸£à¸):")
    print(df[['headline', 'body', 'topic']].head(3).to_string())
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š missing values
    print(f"\nâš ï¸ Missing Values:")
    missing = df[['headline', 'body', 'topic']].isnull().sum()
    for col, count in missing.items():
        print(f"   - {col}: {count}")
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š version (clean/noisy)
    if 'version' in df.columns:
        print(f"\nğŸ” Version Distribution:")
        version_counts = df['version'].value_counts()
        for version, count in version_counts.items():
            print(f"   - {version}: {count:,}")
    
    return df


# ============================================================================
# à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 2: Preprocessing (à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)
# ============================================================================
def preprocess_text(text: str) -> str:
    """
    à¸—à¸³ Preprocessing à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
    
    à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸—à¸³:
    1. Whitespace Normalization - à¸£à¸§à¸¡à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§
    2. Strip - à¸•à¸±à¸”à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸±à¸§à¸—à¹‰à¸²à¸¢
    3. Basic Normalization - à¹à¸›à¸¥à¸‡à¸•à¸±à¸§à¹€à¸¥à¸‚à¹„à¸—à¸¢à¹€à¸›à¹‡à¸™à¸­à¸²à¸£à¸šà¸´à¸ (à¸–à¹‰à¸²à¸¡à¸µ)
    
    à¸‚à¹‰à¸­à¸«à¹‰à¸²à¸¡ (à¹„à¸¡à¹ˆà¸—à¸³ over-cleaning):
    - à¹„à¸¡à¹ˆà¸¥à¸š emoji (à¸­à¸²à¸ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¹ƒà¸™à¸šà¸£à¸´à¸šà¸—à¸‚à¹ˆà¸²à¸§)
    - à¹„à¸¡à¹ˆà¸¥à¸š slang (à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¸«à¸™à¸¶à¹ˆà¸‡à¸‚à¸­à¸‡à¸‚à¹ˆà¸²à¸§)
    - à¹„à¸¡à¹ˆà¸¥à¸šà¸•à¸±à¸§à¹€à¸¥à¸‚ (à¸ªà¸³à¸„à¸±à¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸‚à¹ˆà¸²à¸§)
    - à¹„à¸¡à¹ˆà¸¥à¸šà¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢à¸§à¸£à¸£à¸„à¸•à¸­à¸™ (à¸ªà¸³à¸„à¸±à¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢)
    
    à¹€à¸«à¸•à¸¸à¸œà¸¥:
    - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™ version clean à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ clean à¸¡à¸²à¸
    - à¸à¸²à¸£ over-cleaning à¸­à¸²à¸ˆà¸—à¸³à¹ƒà¸«à¹‰à¸ªà¸¹à¸à¹€à¸ªà¸µà¸¢à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸„à¸±à¸
    - TF-IDF à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š noise à¹„à¸”à¹‰à¹ƒà¸™à¸£à¸°à¸”à¸±à¸šà¸«à¸™à¸¶à¹ˆà¸‡
    
    Args:
        text: à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸•à¹‰à¸™à¸‰à¸šà¸±à¸š
        
    Returns:
        à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸œà¹ˆà¸²à¸™ preprocessing
    """
    if pd.isna(text) or text is None:
        return ""
    
    text = str(text)
    
    # 1. Whitespace Normalization: à¸£à¸§à¸¡à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§
    #    à¹€à¸«à¸•à¸¸à¸œà¸¥: à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸‹à¹‰à¸³à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢ à¹à¸¥à¸°à¸­à¸²à¸ˆà¸—à¸³à¹ƒà¸«à¹‰ TF-IDF à¸™à¸±à¸šà¸„à¸³à¸œà¸´à¸”
    text = re.sub(r'\s+', ' ', text)
    
    # 2. Strip: à¸•à¸±à¸”à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸±à¸§à¸—à¹‰à¸²à¸¢
    #    à¹€à¸«à¸•à¸¸à¸œà¸¥: à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸±à¸§à¸—à¹‰à¸²à¸¢à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢
    text = text.strip()
    
    # 3. Thai Digits Normalization: à¹à¸›à¸¥à¸‡à¸•à¸±à¸§à¹€à¸¥à¸‚à¹„à¸—à¸¢à¹€à¸›à¹‡à¸™à¸­à¸²à¸£à¸šà¸´à¸
    #    à¹€à¸«à¸•à¸¸à¸œà¸¥: à¹ƒà¸«à¹‰à¸•à¸±à¸§à¹€à¸¥à¸‚à¸¡à¸µà¸£à¸¹à¸›à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™
    thai_digits = 'à¹à¹‘à¹’à¹“à¹”à¹•à¹–à¹—à¹˜à¹™'
    arabic_digits = '0123456789'
    for thai, arabic in zip(thai_digits, arabic_digits):
        text = text.replace(thai, arabic)
    
    return text


def prepare_features(df: pd.DataFrame) -> tuple:
    """
    à¹€à¸•à¸£à¸µà¸¢à¸¡ Features à¹à¸¥à¸° Labels
    
    Input Features: headline + " " + body
    Target Label: topic
    
    Args:
        df: DataFrame à¸•à¹‰à¸™à¸‰à¸šà¸±à¸š
        
    Returns:
        tuple à¸‚à¸­à¸‡ (X, y) à¹‚à¸”à¸¢ X à¸„à¸·à¸­ text à¸—à¸µà¹ˆà¸£à¸§à¸¡à¹à¸¥à¹‰à¸§, y à¸„à¸·à¸­ labels
    """
    print("\n" + "=" * 70)
    print("à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 2: Preprocessing (à¸à¸²à¸£à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)")
    print("=" * 70)
    
    print("\nğŸ“‹ à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸à¸²à¸£ Preprocessing:")
    print("   1. Whitespace Normalization - à¸£à¸§à¸¡à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸¥à¸²à¸¢à¸•à¸±à¸§à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§")
    print("   2. Strip - à¸•à¸±à¸”à¸Šà¹ˆà¸­à¸‡à¸§à¹ˆà¸²à¸‡à¸«à¸±à¸§à¸—à¹‰à¸²à¸¢")
    print("   3. Thai Digits Normalization - à¹à¸›à¸¥à¸‡à¸•à¸±à¸§à¹€à¸¥à¸‚à¹„à¸—à¸¢à¹€à¸›à¹‡à¸™à¸­à¸²à¸£à¸šà¸´à¸")
    print("\n   âŒ à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸—à¸³ (à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ over-cleaning):")
    print("   - à¹„à¸¡à¹ˆà¸¥à¸š emoji (à¸­à¸²à¸ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢)")
    print("   - à¹„à¸¡à¹ˆà¸¥à¸š slang (à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¸«à¸™à¸¶à¹ˆà¸‡à¸‚à¸­à¸‡à¸‚à¹ˆà¸²à¸§)")
    print("   - à¹„à¸¡à¹ˆà¸¥à¸šà¸•à¸±à¸§à¹€à¸¥à¸‚à¹à¸¥à¸°à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢à¸§à¸£à¸£à¸„à¸•à¸­à¸™")
    
    # à¸£à¸§à¸¡ headline + body
    df = df.copy()
    df['headline'] = df['headline'].fillna('')
    df['body'] = df['body'].fillna('')
    
    # à¸ªà¸£à¹‰à¸²à¸‡ combined text
    df['text'] = df['headline'] + ' ' + df['body']
    
    # Apply preprocessing
    df['text'] = df['text'].apply(preprocess_text)
    
    X = df['text'].values
    y = df['topic'].values
    
    print(f"\nâœ… à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™:")
    print(f"   - à¸ˆà¸³à¸™à¸§à¸™ samples: {len(X):,}")
    print(f"   - à¸ˆà¸³à¸™à¸§à¸™ classes: {len(np.unique(y))}")
    print(f"   - Classes: {list(np.unique(y))}")
    
    # à¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ text à¸—à¸µà¹ˆà¸£à¸§à¸¡à¹à¸¥à¹‰à¸§
    print(f"\nğŸ“ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ text à¸«à¸¥à¸±à¸‡ preprocessing (100 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¹à¸£à¸):")
    for i in range(min(2, len(X))):
        sample = X[i][:100] + "..." if len(X[i]) > 100 else X[i]
        print(f"   [{i+1}] {sample}")
    
    return X, y, df


# ============================================================================
# à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 3: Baseline Model Training
# ============================================================================
def train_baseline_model(X_train, y_train, X_test, y_test) -> tuple:
    """
    à¸ªà¸£à¹‰à¸²à¸‡à¹à¸¥à¸° Train Baseline Model
    
    Baseline Model Configuration:
    1. Feature Extraction: TF-IDF (word-level)
       - ngram_range=(1, 2): à¹ƒà¸Šà¹‰ unigram à¹à¸¥à¸° bigram
       - max_features=10000: à¸ˆà¸³à¸à¸±à¸”à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¹ƒà¸«à¸à¹ˆà¹€à¸à¸´à¸™à¹„à¸›
       - sublinear_tf=True: à¹ƒà¸Šà¹‰ log scaling à¸ªà¸³à¸«à¸£à¸±à¸š TF
       
    2. Model: Logistic Regression
       - class_weight='balanced': à¸ªà¸³à¸„à¸±à¸! à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š class imbalance
       - max_iter=1000: à¹€à¸à¸´à¹ˆà¸¡ iteration à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ converge
       - solver='lbfgs': à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š multiclass
       
    Args:
        X_train, y_train: Training data
        X_test, y_test: Test data
        
    Returns:
        tuple à¸‚à¸­à¸‡ (vectorizer, model, y_pred)
    """
    print("\n" + "=" * 70)
    print("à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 3: Baseline Model Training")
    print("=" * 70)
    
    # 1. TF-IDF Vectorizer
    print("\nğŸ”§ 1. à¸ªà¸£à¹‰à¸²à¸‡ TF-IDF Vectorizer:")
    print("   - ngram_range: (1, 2) - à¹ƒà¸Šà¹‰ unigram à¹à¸¥à¸° bigram")
    print("   - max_features: 10,000")
    print("   - sublinear_tf: True - à¹ƒà¸Šà¹‰ log scaling")
    
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),      # word-level unigrams and bigrams
        max_features=10000,       # à¸ˆà¸³à¸à¸±à¸”à¸‚à¸™à¸²à¸” vocabulary
        sublinear_tf=True,        # à¹ƒà¸Šà¹‰ 1 + log(tf) à¹à¸—à¸™ tf
        min_df=2,                 # à¸¥à¸°à¹€à¸§à¹‰à¸™à¸„à¸³à¸—à¸µà¹ˆà¸›à¸£à¸²à¸à¸à¸™à¹‰à¸­à¸¢à¸à¸§à¹ˆà¸² 2 à¸„à¸£à¸±à¹‰à¸‡
        max_df=0.95               # à¸¥à¸°à¹€à¸§à¹‰à¸™à¸„à¸³à¸—à¸µà¹ˆà¸›à¸£à¸²à¸à¸à¸¡à¸²à¸à¸à¸§à¹ˆà¸² 95% à¸‚à¸­à¸‡ documents
    )
    
    print("   à¸à¸³à¸¥à¸±à¸‡ fit vectorizer...")
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    
    print(f"   âœ… Vocabulary size: {len(vectorizer.vocabulary_):,}")
    print(f"   âœ… TF-IDF matrix shape: {X_train_tfidf.shape}")
    
    # 2. Logistic Regression
    print("\nğŸ”§ 2. à¸ªà¸£à¹‰à¸²à¸‡ Logistic Regression Model:")
    print("   - class_weight: 'balanced' (à¸ªà¸³à¸„à¸±à¸!)")
    print("   - max_iter: 1000")
    print("   - solver: 'lbfgs'")
    print("   - multi_class: 'multinomial'")
    
    model = LogisticRegression(
        class_weight='balanced',  # à¸ªà¸³à¸„à¸±à¸! à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š class imbalance
        max_iter=1000,
        solver='lbfgs',
        random_state=42,
        n_jobs=-1
    )
    
    print("   à¸à¸³à¸¥à¸±à¸‡ train model...")
    model.fit(X_train_tfidf, y_train)
    print("   âœ… Training à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™!")
    
    # 3. Prediction
    y_pred = model.predict(X_test_tfidf)
    
    return vectorizer, model, y_pred


def save_model(vectorizer, model, output_dir: str):
    """
    à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œ .joblib
    
    Args:
        vectorizer: TF-IDF vectorizer
        model: Trained model
        output_dir: à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¸šà¸±à¸™à¸—à¸¶à¸
    """
    print("\nğŸ’¾ à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥:")
    
    # à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ
    os.makedirs(output_dir, exist_ok=True)
    
    # à¸šà¸±à¸™à¸—à¸¶à¸ vectorizer
    vectorizer_path = os.path.join(output_dir, 'tfidf_vectorizer.joblib')
    joblib.dump(vectorizer, vectorizer_path)
    print(f"   âœ… Vectorizer saved: {vectorizer_path}")
    
    # à¸šà¸±à¸™à¸—à¸¶à¸ model
    model_path = os.path.join(output_dir, 'logistic_regression_model.joblib')
    joblib.dump(model, model_path)
    print(f"   âœ… Model saved: {model_path}")
    
    return vectorizer_path, model_path


# ============================================================================
# à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 4: Evaluation (à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥)
# ============================================================================
def evaluate_model(y_test, y_pred, classes, output_dir: str = None) -> dict:
    """
    à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹‚à¸¡à¹€à¸”à¸¥
    
    à¸£à¸²à¸¢à¸‡à¸²à¸™:
    - Accuracy
    - Macro-F1
    - Confusion Matrix
    
    Args:
        y_test: Ground truth labels
        y_pred: Predicted labels
        classes: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ classes
        output_dir: à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¸šà¸±à¸™à¸—à¸¶à¸à¸£à¸¹à¸› (optional)
        
    Returns:
        dict à¸‚à¸­à¸‡ metrics
    """
    print("\n" + "=" * 70)
    print("à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 4: Evaluation (à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥)")
    print("=" * 70)
    
    # 1. Accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nğŸ“Š Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # 2. Macro-F1
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    print(f"ğŸ“Š Macro-F1: {macro_f1:.4f}")
    
    # 3. Classification Report
    print("\nğŸ“‹ Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # 4. Confusion Matrix
    cm = confusion_matrix(y_test, y_pred, labels=classes)
    print("\nğŸ“Š Confusion Matrix:")
    print(pd.DataFrame(cm, index=classes, columns=classes).to_string())
    
    # Plot confusion matrix
    if output_dir:
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=classes,
            yticklabels=classes
        )
        plt.title('Confusion Matrix - Thai News Topic Classifier')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.tight_layout()
        
        cm_path = os.path.join(output_dir, 'confusion_matrix.png')
        plt.savefig(cm_path, dpi=150)
        print(f"\nğŸ’¾ Confusion Matrix saved: {cm_path}")
        plt.close()
    
    return {
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'confusion_matrix': cm
    }


def get_misclassified_samples(df_test, y_test, y_pred, n_samples: int = 10) -> pd.DataFrame:
    """
    à¸”à¸¶à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¹‚à¸¡à¹€à¸”à¸¥à¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”
    
    Args:
        df_test: DataFrame à¸‚à¸­à¸‡ test set
        y_test: Ground truth labels
        y_pred: Predicted labels
        n_samples: à¸ˆà¸³à¸™à¸§à¸™à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
        
    Returns:
        DataFrame à¸‚à¸­à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”
    """
    print(f"\nğŸ“ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸” (à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ {n_samples} à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡):")
    print("-" * 70)
    
    # à¸«à¸² indices à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”
    wrong_mask = y_test != y_pred
    wrong_indices = np.where(wrong_mask)[0]
    
    print(f"   à¸ˆà¸³à¸™à¸§à¸™à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {len(wrong_indices)} à¸ˆà¸²à¸ {len(y_test)}")
    print(f"   à¸­à¸±à¸•à¸£à¸²à¸œà¸´à¸”à¸à¸¥à¸²à¸”: {len(wrong_indices)/len(y_test)*100:.2f}%")
    
    # à¸ªà¸¸à¹ˆà¸¡à¹€à¸¥à¸·à¸­à¸à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡
    sample_indices = wrong_indices[:min(n_samples, len(wrong_indices))]
    
    # à¸ªà¸£à¹‰à¸²à¸‡ DataFrame
    misclassified = []
    for i, idx in enumerate(sample_indices):
        row = df_test.iloc[idx]
        headline = row['headline'][:50] + "..." if len(str(row['headline'])) > 50 else row['headline']
        body = row['body'][:100] + "..." if len(str(row['body'])) > 100 else row['body']
        
        misclassified.append({
            'index': idx,
            'headline': headline,
            'body': body,
            'actual': y_test[idx],
            'predicted': y_pred[idx]
        })
        
        print(f"\n   [{i+1}] Index: {idx}")
        print(f"       Headline: {headline}")
        print(f"       Body: {body}")
        print(f"       Actual: {y_test[idx]} â†’ Predicted: {y_pred[idx]}")
    
    return pd.DataFrame(misclassified)


# ============================================================================
# à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 5: Error Analysis (à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”)
# ============================================================================
def analyze_errors(misclassified_df: pd.DataFrame, classes: list):
    """
    à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¹à¸¥à¸°à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡
    
    à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”à¸—à¸µà¹ˆà¸à¸šà¸šà¹ˆà¸­à¸¢:
    1. Mixed Signal (à¸„à¸§à¸²à¸¡à¸à¸³à¸à¸§à¸¡à¸‚à¸­à¸‡à¸ à¸²à¸©à¸²)
       - à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¸¡à¸µà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸—à¸±à¸šà¸‹à¹‰à¸­à¸™à¸«à¸¥à¸²à¸¢à¸«à¸¡à¸§à¸”
       - à¹€à¸Šà¹ˆà¸™ à¸‚à¹ˆà¸²à¸§ Business à¸—à¸µà¹ˆà¸à¸¹à¸”à¸–à¸¶à¸‡ SciTech
       
    2. Domain Shift (à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¹€à¸‰à¸à¸²à¸°à¸—à¸²à¸‡)
       - à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¸—à¸µà¹ˆà¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¹€à¸„à¸¢à¹€à¸«à¹‡à¸™à¹ƒà¸™ training
       - à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¸•à¹ˆà¸²à¸‡à¹ƒà¸™à¸šà¸£à¸´à¸šà¸—à¸•à¹ˆà¸²à¸‡à¸à¸±à¸™
       
    3. Typo/Noise (à¸›à¸±à¸à¸«à¸²à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)
       - à¸•à¸±à¸§à¸ªà¸°à¸à¸”à¸œà¸´à¸”
       - à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ
       
    Args:
        misclassified_df: DataFrame à¸‚à¸­à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”
        classes: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ classes
    """
    print("\n" + "=" * 70)
    print("à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 5: Error Analysis (à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”)")
    print("=" * 70)
    
    print("\nğŸ” à¸à¸²à¸£à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¹‰à¸­à¸œà¸´à¸”à¸à¸¥à¸²à¸”:")
    print("\n" + "-" * 60)
    
    # 1. Mixed Signal Analysis
    print("\nğŸ“Œ à¸›à¸£à¸°à¹€à¸ à¸—à¸—à¸µà¹ˆ 1: Mixed Signal (à¸„à¸§à¸²à¸¡à¸à¸³à¸à¸§à¸¡à¸‚à¸­à¸‡à¸ à¸²à¸©à¸²)")
    print("   à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢: à¸‚à¹ˆà¸²à¸§à¸—à¸µà¹ˆà¸¡à¸µà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸—à¸±à¸šà¸‹à¹‰à¸­à¸™à¸«à¸¥à¸²à¸¢à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ")
    print("   à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: à¸‚à¹ˆà¸²à¸§ Business à¸—à¸µà¹ˆà¸à¸¹à¸”à¸–à¸¶à¸‡à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µ à¸­à¸²à¸ˆà¸–à¸¹à¸à¸—à¸³à¸™à¸²à¸¢à¹€à¸›à¹‡à¸™ SciTech")
    
    # à¸™à¸±à¸š confusion pairs
    if len(misclassified_df) > 0:
        confusion_pairs = misclassified_df.groupby(['actual', 'predicted']).size()
        print("\n   à¸„à¸¹à¹ˆà¸—à¸µà¹ˆà¸ªà¸±à¸šà¸ªà¸™à¸šà¹ˆà¸­à¸¢:")
        for (actual, predicted), count in confusion_pairs.head(5).items():
            print(f"   - {actual} â†’ {predicted}: {count} à¸„à¸£à¸±à¹‰à¸‡")
    
    # 2. Domain Shift
    print("\nğŸ“Œ à¸›à¸£à¸°à¹€à¸ à¸—à¸—à¸µà¹ˆ 2: Domain Shift (à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¹€à¸‰à¸à¸²à¸°à¸—à¸²à¸‡)")
    print("   à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢: à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¹€à¸‰à¸à¸²à¸°à¸—à¸²à¸‡à¸—à¸µà¹ˆà¹‚à¸¡à¹€à¸”à¸¥à¹„à¸¡à¹ˆà¸„à¸¸à¹‰à¸™à¹€à¸„à¸¢")
    print("   à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡: ")
    print("   - à¸„à¸³à¸¨à¸±à¸à¸—à¹Œà¸—à¸²à¸‡à¸à¸²à¸£à¹€à¸‡à¸´à¸™à¹ƒà¸«à¸¡à¹ˆà¹†")
    print("   - à¸Šà¸·à¹ˆà¸­à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸¥à¹ˆà¸²à¸ªà¸¸à¸”")
    print("   - à¸¨à¸±à¸à¸—à¹Œà¹€à¸‰à¸à¸²à¸°à¸—à¸²à¸‡à¸à¸²à¸£à¸—à¸¹à¸•")
    
    # 3. Typo/Noise
    print("\nğŸ“Œ à¸›à¸£à¸°à¹€à¸ à¸—à¸—à¸µà¹ˆ 3: Typo/Noise (à¸›à¸±à¸à¸«à¸²à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)")
    print("   à¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸›à¸à¸•à¸´")
    print("   à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡:")
    print("   - à¸•à¸±à¸§à¸ªà¸°à¸à¸”à¸œà¸´à¸”")
    print("   - headline à¸«à¸£à¸·à¸­ body à¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸›")
    print("   - à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ")
    
    # Recommendations
    print("\n" + "=" * 60)
    print("ğŸ’¡ à¹à¸™à¸§à¸—à¸²à¸‡à¹à¸à¹‰à¹„à¸‚ (Recommendations):")
    print("=" * 60)
    
    print("\n1. à¸ªà¸³à¸«à¸£à¸±à¸š Mixed Signal:")
    print("   - à¹€à¸à¸´à¹ˆà¸¡ features à¸ˆà¸²à¸ subtopic à¹€à¸à¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¹à¸¢à¸à¹à¸¢à¸°")
    print("   - à¹ƒà¸Šà¹‰ hierarchical classification")
    print("   - à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰ multi-label classification")
    
    print("\n2. à¸ªà¸³à¸«à¸£à¸±à¸š Domain Shift:")
    print("   - à¹€à¸à¸´à¹ˆà¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ training à¸—à¸µà¹ˆà¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢")
    print("   - à¹ƒà¸Šà¹‰ word embeddings à¹à¸—à¸™ TF-IDF")
    print("   - à¹ƒà¸Šà¹‰ pre-trained Thai language models (à¹€à¸Šà¹ˆà¸™ WangchanBERTa)")
    
    print("\n3. à¸ªà¸³à¸«à¸£à¸±à¸š Typo/Noise:")
    print("   - à¹€à¸à¸´à¹ˆà¸¡à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™ spell checking")
    print("   - à¹ƒà¸Šà¹‰ character-level features à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡")
    print("   - à¸à¸£à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸ªà¸±à¹‰à¸™à¹€à¸à¸´à¸™à¹„à¸›à¸­à¸­à¸")
    
    print("\nğŸ’¡ à¹à¸™à¸§à¸—à¸²à¸‡à¹à¸à¹‰à¹„à¸‚à¸«à¸¥à¸±à¸ (à¹€à¸ªà¸™à¸­ 1 à¹à¸™à¸§à¸—à¸²à¸‡):")
    print("-" * 60)
    print("""
    ğŸ¯ à¹ƒà¸Šà¹‰ Pre-trained Thai Language Model (WangchanBERTa)
    
    à¹€à¸«à¸•à¸¸à¸œà¸¥:
    1. à¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¹€à¸Šà¸´à¸‡à¸šà¸£à¸´à¸šà¸— (contextual understanding)
    2. à¸ˆà¸±à¸”à¸à¸²à¸£à¸à¸±à¸š Mixed Signal à¹„à¸”à¹‰à¸”à¸µà¸à¸§à¹ˆà¸² TF-IDF
    3. à¸—à¸™à¸•à¹ˆà¸­ typo à¹à¸¥à¸° noise à¹„à¸”à¹‰à¸”à¸µà¸à¸§à¹ˆà¸²
    4. à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸—à¸³ feature engineering à¸¡à¸²à¸
    
    à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸à¸²à¸£ implement:
    - pip install transformers pythainlp
    - à¹ƒà¸Šà¹‰ model: airesearch/wangchanberta-base-att-spm-uncased
    - Fine-tune à¸šà¸™ dataset à¸™à¸µà¹‰
    """)


# ============================================================================
# Main Function
# ============================================================================
def main():
    """
    Main function à¸ªà¸³à¸«à¸£à¸±à¸š train Thai News Topic Classifier
    """
    print("=" * 70)
    print("ğŸ‡¹ğŸ‡­ Thai News Topic Classifier")
    print("=" * 70)
    
    # Configuration
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    CSV_PATH = os.path.join(SCRIPT_DIR, '12.agnews_thai_train_easy.csv')
    OUTPUT_DIR = os.path.join(SCRIPT_DIR, 'output')
    TEST_SIZE = 0.2
    RANDOM_STATE = 42
    
    print(f"\nâš™ï¸ Configuration:")
    print(f"   - CSV Path: {CSV_PATH}")
    print(f"   - Output Dir: {OUTPUT_DIR}")
    print(f"   - Test Size: {TEST_SIZE}")
    print(f"   - Random State: {RANDOM_STATE}")
    
    # à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 1: Load and understand dataset
    df = load_and_understand_dataset(CSV_PATH)
    
    # à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 2: Preprocess data
    X, y, df_processed = prepare_features(df)
    
    # à¹à¸šà¹ˆà¸‡ Train/Test
    print("\n" + "=" * 70)
    print("à¸à¸²à¸£à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train/Test")
    print("=" * 70)
    
    # à¸ªà¸£à¹‰à¸²à¸‡ indices à¸ªà¸³à¸«à¸£à¸±à¸š split
    indices = np.arange(len(X))
    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(
        X, y, indices,
        test_size=TEST_SIZE,
        random_state=RANDOM_STATE,
        stratify=y  # à¸£à¸±à¸à¸©à¸²à¸ªà¸±à¸”à¸ªà¹ˆà¸§à¸™ class
    )
    
    print(f"\nğŸ“Š à¸à¸²à¸£à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:")
    print(f"   - Training set: {len(X_train):,} samples ({(1-TEST_SIZE)*100:.0f}%)")
    print(f"   - Test set: {len(X_test):,} samples ({TEST_SIZE*100:.0f}%)")
    
    # à¸ªà¸£à¹‰à¸²à¸‡ df_test à¸ªà¸³à¸«à¸£à¸±à¸š error analysis
    df_test = df_processed.iloc[idx_test].reset_index(drop=True)
    
    # à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 3: Train baseline model
    vectorizer, model, y_pred = train_baseline_model(X_train, y_train, X_test, y_test)
    
    # à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥
    save_model(vectorizer, model, OUTPUT_DIR)
    
    # à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 4: Evaluate model
    classes = sorted(df['topic'].unique())
    metrics = evaluate_model(y_test, y_pred, classes, OUTPUT_DIR)
    
    # à¸”à¸¶à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢à¸œà¸´à¸”
    misclassified_df = get_misclassified_samples(df_test, y_test, y_pred, n_samples=10)
    
    # à¸šà¸±à¸™à¸—à¸¶à¸ misclassified samples
    if len(misclassified_df) > 0:
        misclassified_path = os.path.join(OUTPUT_DIR, 'misclassified_samples.csv')
        misclassified_df.to_csv(misclassified_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ Misclassified samples saved: {misclassified_path}")
    
    # à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆ 5: Error Analysis
    analyze_errors(misclassified_df, classes)
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š Summary")
    print("=" * 70)
    print(f"\n   âœ… Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"   âœ… Macro-F1: {metrics['macro_f1']:.4f}")
    print(f"\n   ğŸ“ Output files saved to: {OUTPUT_DIR}")
    print(f"      - tfidf_vectorizer.joblib")
    print(f"      - logistic_regression_model.joblib")
    print(f"      - confusion_matrix.png")
    print(f"      - misclassified_samples.csv")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ Training Complete!")
    print("=" * 70)


if __name__ == "__main__":
    main()
